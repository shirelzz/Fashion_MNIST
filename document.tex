\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{pifont}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{mdframed}

\definecolor{light-gray}{gray}{0.95}
\newcommand{\code}[1]{\colorbox{light-gray}{\texttt{#1}}}
% \geometry{margin=1.5in}

\title{Deep Learning and Natural Language Processing Project}
\author{Moria Grohar, Shirel Zecharia}
\date{\today}

\begin{document}

\maketitle

\section{Abstract}
In this report, we present a study on implementing a neural network model for the classification of fashion items using the Fashion MNIST dataset.\\
We compare the performance of the neural network approach with traditional machine learning algorithms such as Logistic Regression, and Multi-Layer Perceptron (MLP).\\
The report discusses the project methodology, experimental results, and conclusions drawn from the comparison.

\section{Introduction}
Your introduction text here.

\section{Related Work and Required Background}

\subsection{Softmax Regression}
Softmax regression, also known as multinomial logistic regression, is a generalization of logistic regression.
While in logistic regression we assumed that the labels were binary: ${y(i)\in{0,1}}$, In Softmax regression we are able to handle multiple labels ${y(i)\in{1,â€¦,K}}$ where ${k}$ is the number of classes.\\

In softmax regression, the goal is to predict the probability that an input sample belongs to each possible class. This is achieved by applying the softmax function to the output of a linear model. The linear model computes a score for each class based on the input features, and the softmax function converts these scores into probabilities.\\

Mathematically, given an input vector ${x}$, softmax regression computes the probability ${P(y=i | x)}$ that the input belongs to class ${i}$ as follows:
$${P(y=i | x) = \frac{e^{x^{T} \cdot W_i + b_i}}{\sum_{j=1}^{k} e^{x^{T} \cdot W_j + b_j}}
}$$

\noindent\hspace*{10mm}
\begin{minipage}{\dimexpr\linewidth-20mm}
Where:\\
${y}$ is the target variable (class).\\
${x}$ is the vector of input features.\\
${T}$ denotes the transpose applied to the vector.\\
${W_i}$: the weight vector associated with class ${i}$.\\
${b_i}$: the bias term associated with class ${i}$.\\
\end{minipage}\\
\textbf{Understanding The Process:}\hfill\newline\\
Linear Transformation: For each class, it computes a linear combination of the input features using class-specific weights. This generates a score for each class, indicating its "fit" for the data point.\hfill\newline\\
Softmax Function: This function takes the vector of scores and transforms them into probabilities. It ensures:
\begin{itemize}
\item Non-negativity: All probabilities are positive (between 0 and 1).
\end{itemize}
\begin{itemize}
\item Normalization: The sum of all probabilities for a single data point is always 1, representing a valid probability distribution.
\end{itemize}
The softmax function is often used as the final activation layer in multi-class neural networks as we will discuss later in the article.

\subsection{Activation Functions}
Activation functions are mathematical functions used to determine the output of a neuron, influencing how information flows through the network and ultimately shaping its learning and decision-making capabilities.\\\\
Traditional neural networks without activation functions would simply perform linear transformations on the input data. This limitation restricts them from learning complex patterns and relationships in the data, hindering their ability to perform tasks like image recognition or natural language processing.\\\\
A neuron in a neural network receives multiple inputs, each carrying a specific value. These inputs are combined through weighted connections and then passed through an activation function. The function acts like a filter, transforming the combined input into a single output value. This output value then becomes the basis for the neuron's activation, influencing the signals it transmits to other neurons in the network.\\\\
The selection of an appropriate activation function depends on the specific task and network architecture. While ReLU and Leaky ReLU are popular choices due to their efficiency, other functions like sigmoid and tanh might be suitable for specific scenarios like output normalization or recurrent neural networks.\newline\newline
\textbf{Popular Activation Functions:}\hfill\newline\\

\begin{itemize}
\item[\ding{118}] Rectified Linear Unit:
ReLU (Rectified Linear Unit) activation functions serve as the non-linear activation function of choice in most CNN architectures. ReLU introduces non-linearity into the network, enabling it to learn complex mappings between the input and output. Mathematically, ReLU simply outputs the input if it is positive and zero otherwise, which simplifies computations and accelerates convergence during training.
\end{itemize}

\begin{itemize}
\item[\ding{118}] Leaky Rectified Linear Unit:
Leaky ReLU is a variant of the ReLU activation function that addresses the "dying ReLU" problem, where neurons may become inactive and cease to update their weights during training. Unlike ReLU, which sets negative values to zero, Leaky ReLU allows a small, non-zero gradient for negative inputs. This modification ensures that neurons remain active and continue to contribute to the learning process, especially in deeper networks where vanishing gradients are more prevalent.
\end{itemize}

\begin{itemize}
\item[\ding{118}] Sigmoid:
This function squishes the input values between 0 and 1, mimicking the firing behavior of biological neurons. However, its vanishing gradients can hinder learning in deep networks.
\end{itemize}

\begin{itemize}
\item[\ding{118}] Tanh (Hyperbolic tangent):
Similar to sigmoid, tanh maps the input values to a range of -1 to 1. It offers slightly better gradients than sigmoid but suffers from similar limitations.
\end{itemize}

\subsection{Recurrent Neural Networks}


\subsection{Convolutional Neural Networks}
Convolutional Neural Networks (CNNs) are a specific type of artificial neural network architecture particularly well-suited for analyzing image data. They excel at tasks like image classification, object detection, and image segmentation. Unlike traditional neural networks, CNNs incorporate an additional layer type called a convolutional layer to extract features from the input data. The essence of a CNN lies in its ability to automatically learn spatial hierarchies of features from raw data. These networks consist of several layers, typically including convolutional layers, pooling layers, and fully connected layers.

\subsubsection{Convolutional Layers}
Convolutional layers are the cornerstone of CNNs, responsible for learning and detecting patterns within the input data. They employ mathematical operations known as convolutions to extract features such as edges, textures, and other distinctive attributes from the input images. Mathematically, convolution involves sliding a filter or a kernel over the input data, performing element-wise multiplications, and summing the results to produce feature maps.

\subsubsection{Pooling Layers}
Pooling layers reduce the spatial dimensions of feature maps generated by convolutional layers while retaining important information. Common pooling techniques include max pooling and average pooling, where the former selects the maximum value from a region of the feature map, while the latter computes the average. Pooling helps in achieving translation invariance and reducing the computational burden by downsampling the feature maps.

\subsubsection{Fully Connected Layers}
These layers function similarly to traditional neural networks, connecting all neurons in one layer to all neurons in the next. They process the extracted features and learn complex relationships to form the final output, such as a class prediction or a set of bounding boxes for object detection.

\subsection{Optimizers}
Optimizers are used for updating the model's parameters to minimize the loss function during the training process, which ultimately leads to better performance of the model.

\subsubsection{Stochastic Gradient Descent (SGD)}
The SGD algorithm takes a single data point (or a small batch) at a time, calculates the gradient (the direction of steepest descent in the loss function), and updates the model's parameters in the opposite direction by a small learning rate. This technique is computationally inexpensive, making it suitable for large datasets. But on the other hand it may require many iterations to reach an optimal solution, especially for complex models and datasets.\\
The update rule for SGD can be represented as:
$${\theta_{t+1} = \theta_t - \eta \cdot \nabla L(\theta_t)}$$

\noindent\hspace*{10mm}
\begin{minipage}{\dimexpr\linewidth-20mm}
Where:\\
${\theta_t}$ is the current parameter values.\\
${\eta}$ is the learning rate, a small positive scalar determining the step size.\\
${\nabla L(\theta_t)}$ is the gradient of the function ${L}$ at ${\theta_t}$, which points in the direction of the steepest increase in ${L}$.
\end{minipage}

\subsubsection{Momentum SGD}
SGD with Momentum is used to improve the performance of the neural network. accelerates convergence, especially in the context of high curvature, small but consistent gradients, or noisy gradients. It does this by adding a fraction ${\gamma}$ of the update vector of the past time step to the current update.\\
The update rule for Momentum SGD is:
\begin{align*}
v_{t} & = \gamma \cdot v_{t-1} + \eta \nabla L(\theta_t)\\
\theta_{t+1} & = \theta_t - v_{t}
\end{align*}

\noindent\hspace*{10mm}
\begin{minipage}{\dimexpr\linewidth-20mm}
Where:\\
$v_{t}$ is the velocity at time step ${t}$, which is a combination of the current gradient and the velocity from the previous step $v_{t-1}$.\\
${\gamma}$ (often set between 0.9 and 0.99) is the momentum term, determining how much of the past velocity to add to the current step.
\end{minipage}

\subsubsection{RMSprop (Root Mean Square Propagation)}
RMSprop addresses a limitation of SGD, where frequent updates based on noisy gradients can lead to slow convergence.
Its key Feature is that it incorporates an exponential moving average of the squared gradients for each parameter. This averages the recent updates and reduces the impact of large, infrequent gradients, leading to smoother convergence. This technique has an increased computational cost compared to SGD and RMSprop.This technique has an increased computational cost compared to SGD.

\begin{align*}
s_{t+1} & = \beta \cdot s_t + (1 - \beta) \cdot (\nabla L(\theta_t))^2\\
\theta_{t+1} & = \theta_t - \frac{\eta}{\sqrt{s_{t+1}} + \epsilon} \cdot \nabla L(\theta_t)
\end{align*}

\subsubsection{Adam (Adaptive Moment Estimation)}
The Adam optimizer combines ideas from SGD, RMSprop, and momentum, aiming to adapt the learning rate for each parameter individually and accelerate convergence.
It maintains exponential moving averages of gradients (similar to RMSprop) and squared gradients.
It incorporates momentum to account for the history of past updates, addressing the issue of getting stuck in local minima.
It adaptively adjusts the learning rate for each parameter based on these estimates, aiming for optimal updates. This technique has an increased computational cost compared to SGD and RMSprop.
\begin{align*}
m_{t+1} & = \beta_1 \cdot m_t + (1 - \beta_1) \cdot \nabla L(\theta_t) \\
v_{t+1} & = \beta_2 \cdot v_t + (1 - \beta_2) \cdot (\nabla L(\theta_t))^2 \\
\hat{m}_{t+1} & = \frac{m_{t+1}}{1 - \beta_1^{t+1}} \\
\hat{v}_{t+1} & = \frac{v_{t+1}}{1 - \beta_2^{t+1}} \\
\theta_{t+1} & = \theta_t - \frac{\eta}{\sqrt{\hat{v}_{t+1}} + \epsilon} \cdot \hat{m}_{t+1}
\end{align*}


\section{Previous Attempts}
Your previous attempts description text here.

\section{Project Description}
Your project description text here.

\section{Simulation Results}

\subsection{Softmax Regression Model}

\begin{figure}[h]
\caption{Training history of the data}
\centering
\includegraphics[width=8cm]{Figure_relu.png}
\end{figure}

\subsection{RNN Model}

\begin{figure}[h]
\caption{Training history of the data}
\centering
\includegraphics[width=8cm]{Figure_relu.png}
\end{figure}

\subsection{CNN Model}

\begin{figure}[h]
\caption{Training history of the data}
\centering
\includegraphics[width=8cm]{Figure_relu.png}
\end{figure}
\newpage

\section{Code}
\subsection{CNN Model}
\code{trainCNNModel}
\newline
\newline
Convolutional Layers: These layers apply a convolution operation to the input image,
extracting various features through filters.
The activation function used is ReLU,
which introduces non-linearity into the model, enabling it to learn complex patterns.\\\newline
Batch Normalization: This technique is used to improve the training stability and speed
by normalizing the inputs of each layer.\\\newline
MaxPooling Layers: Max pooling reduces the spatial dimensions of the feature maps,
effectively downsampling the input.
It retains the most important features while reducing computational
complexity and preventing overfitting.\\\newline
Flatten Layer: This layer flattens the 2D feature maps into a 1D vector,
preparing the data for input into a fully connected neural network.\\\newline
Dense Layers: Fully connected layers that perform classification based on the learned features.
The first dense layer has 64 units with ReLU activation, enabling the network to learn complex
patterns in the flattened feature vectors.
The final dense layer has units equal to the number of classes
in the dataset, with softmax activation, which outputs probabilities for each class,
indicating the likelihood of the input image belonging to each class.\\\newline
Dropout: This layer was added to prevent overfitting.
It works by randomly setting a fraction of input units to zero during training,
which helps to prevent the network from relying too much on specific neurons
and encourages it to learn more robust features.
We've defined a Dropout layer with a dropout rate of 0.2.
This means during training, ${20\%}$ of the input units to the Dropout layer will be randomly set to zero.

\textbf{Code}\hfill\newline\\

\code{def createCNNModel(inputShape, numClasses):}\newline


\code{2nd func:}\newline

earlystopping


\section{Conclusion}
Your conclusion text here.

\end{document}