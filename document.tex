\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\title{Deep Learning and Natural Language Processing Project}
\author{Moria Grohar, Shirel Zecharia}
\date{\today}

\begin{document}

\maketitle

\section{Abstract}
In this report, we present a study on implementing a neural network model for the classification of fashion items using the Fashion MNIST dataset.\\
We compare the performance of the neural network approach with traditional machine learning algorithms such as Logistic Regression, and Multi-Layer Perceptron (MLP).\\
The report discusses the project methodology, experimental results, and conclusions drawn from the comparison.

\section{Introduction}
Your introduction text here.

\section{Related Work and Required Background}

\section{Activation Functions}
Activation functions play a crucial role, acting as the gatekeepers within artificial neural networks. These mathematical functions determine the output of a neuron, influencing how information flows through the network and ultimately shaping its learning and decision-making capabilities.\\
Traditional neural networks without activation functions would simply perform linear transformations on the input data. This limitation restricts them from learning complex patterns and relationships in the data, hindering their ability to perform tasks like image recognition or natural language processing.\\
A neuron in a neural network receives multiple inputs, each carrying a specific value. These inputs are combined through weighted connections and then passed through an activation function. The function acts like a filter, transforming the combined input into a single output value. This output value then becomes the basis for the neuron's activation, influencing the signals it transmits to other neurons in the network.

\subsection{Popular Activation Functions:}
\subsubsection{Rectified Linear Unit}
ReLU (Rectified Linear Unit) activation functions serve as the non-linear activation function of choice in most CNN architectures. ReLU introduces non-linearity into the network, enabling it to learn complex mappings between the input and output. Mathematically, ReLU simply outputs the input if it is positive and zero otherwise, which simplifies computations and accelerates convergence during training.

\subsubsection{Leaky Rectified Linear Unit}
Leaky ReLU is a variant of the ReLU activation function that addresses the "dying ReLU" problem, where neurons may become inactive and cease to update their weights during training. Unlike ReLU, which sets negative values to zero, Leaky ReLU allows a small, non-zero gradient for negative inputs. This modification ensures that neurons remain active and continue to contribute to the learning process, especially in deeper networks where vanishing gradients are more prevalent.

\subsubsection{Sigmoid}
This function squishes the input values between 0 and 1, mimicking the firing behavior of biological neurons. However, its vanishing gradients can hinder learning in deep networks.

\subsubsection{Tanh (Hyperbolic tangent)}
Similar to sigmoid, tanh maps the input values to a range of -1 to 1. It offers slightly better gradients than sigmoid but suffers from similar limitations.

\subsection{Convolutional Neural Networks}
Convolutional Neural Networks (CNNs) are a specific type of artificial neural network architecture particularly well-suited for analyzing image data. They excel at tasks like image classification, object detection, and image segmentation. Unlike traditional neural networks, CNNs incorporate an additional layer type called a convolutional layer to extract features from the input data. The essence of a CNN lies in its ability to automatically learn spatial hierarchies of features from raw data. These networks consist of several layers, typically including convolutional layers, pooling layers, and fully connected layers.

\subsubsection{Convolutional Layers}
Convolutional layers are the cornerstone of CNNs, responsible for learning and detecting patterns within the input data. They employ mathematical operations known as convolutions to extract features such as edges, textures, and other distinctive attributes from the input images. Mathematically, convolution involves sliding a filter or a kernel over the input data, performing element-wise multiplications, and summing the results to produce feature maps.

\subsubsection{Pooling Layers}
Pooling layers play a crucial role in reducing the spatial dimensions of feature maps generated by convolutional layers while retaining important information. Common pooling techniques include max pooling and average pooling, where the former selects the maximum value from a region of the feature map, while the latter computes the average. Pooling helps in achieving translation invariance and reducing the computational burden by downsampling the feature maps.

\subsubsection{Fully Connected Layers}
These layers function similarly to traditional neural networks, connecting all neurons in one layer to all neurons in the next. They process the extracted features and learn complex relationships to form the final output, such as a class prediction or a set of bounding boxes for object detection.


\section{Previous Attempts}
Your previous attempts description text here.

\section{Project Description}
Your project description text here.

\section{Simulation Results}
Your experiments/simulation results text here.

\section{Code}
CNN:
    The CNN architecture defined in this function follows a common pattern used for image classification tasks.
    It starts with convolutional layers that learn low-level features such as edges and textures.
    As the network deepens, subsequent layers learn increasingly complex and abstract features.
    Max pooling layers help in reducing spatial dimensions while retaining important features.
    Finally, the flattened feature vectors are fed into fully connected layers for classification.
    The use of ReLU activation functions helps in introducing non-linearity,
    and the softmax activation in the output layer enables the model to output class probabilities,
    making it suitable for multi-class classification tasks.
    Overall, this CNN architecture efficiently learns hierarchical representations from image data,
    enabling accurate classification.

\section{Conclusion}
Your conclusion text here.

\end{document}
